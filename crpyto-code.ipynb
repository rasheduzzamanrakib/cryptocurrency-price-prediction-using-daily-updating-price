{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12791677,"sourceType":"datasetVersion","datasetId":7855118}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-18T14:03:52.540806Z","iopub.execute_input":"2025-08-18T14:03:52.541630Z","iopub.status.idle":"2025-08-18T14:03:52.550283Z","shell.execute_reply.started":"2025-08-18T14:03:52.541603Z","shell.execute_reply":"2025-08-18T14:03:52.549102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T14:03:52.551633Z","iopub.execute_input":"2025-08-18T14:03:52.552582Z","iopub.status.idle":"2025-08-18T14:03:52.567801Z","shell.execute_reply.started":"2025-08-18T14:03:52.552556Z","shell.execute_reply":"2025-08-18T14:03:52.566883Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/kaggle/input/daily-crypto-tracker-dataset/daily_crypto_tracker.csv\")\nprint(df.head())\nprint(df.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T14:03:52.569005Z","iopub.execute_input":"2025-08-18T14:03:52.569435Z","iopub.status.idle":"2025-08-18T14:03:52.598631Z","shell.execute_reply.started":"2025-08-18T14:03:52.569409Z","shell.execute_reply":"2025-08-18T14:03:52.597930Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_clean = df.drop(columns=['price_change_percentage_24h', 'fetch_date', 'id', 'symbol'])\ndf_clean = df_clean.dropna()\ndf_clean['is_gain'] = (\n    df_clean['current_price'] > df_clean['current_price'].shift(1).fillna(df_clean['current_price'])\n).astype(int)\n\ndf_clean = df_clean.dropna()\nX = df_clean.drop(columns=['is_gain'])\ny = df_clean['is_gain']\nprint(\"âœ… Features ready:\", X.shape)\nprint(\"ðŸŽ¯ Target ready:\", y.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T14:03:52.600180Z","iopub.execute_input":"2025-08-18T14:03:52.600423Z","iopub.status.idle":"2025-08-18T14:03:52.611376Z","shell.execute_reply.started":"2025-08-18T14:03:52.600405Z","shell.execute_reply":"2025-08-18T14:03:52.610457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T14:03:52.612187Z","iopub.execute_input":"2025-08-18T14:03:52.612457Z","iopub.status.idle":"2025-08-18T14:03:52.631012Z","shell.execute_reply.started":"2025-08-18T14:03:52.612436Z","shell.execute_reply":"2025-08-18T14:03:52.630272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\nprint(\"ðŸ“‚ Train size:\", X_train.shape)\nprint(\"ðŸ“‚ Test size:\", X_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T14:03:52.631975Z","iopub.execute_input":"2025-08-18T14:03:52.632266Z","iopub.status.idle":"2025-08-18T14:03:52.647692Z","shell.execute_reply.started":"2025-08-18T14:03:52.632237Z","shell.execute_reply":"2025-08-18T14:03:52.646792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#1_Random Forest\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Encode categorical features in X_train and X_test\nX_train_encoded = pd.get_dummies(X_train)\nX_test_encoded = pd.get_dummies(X_test)\n\n# Align train and test encoded data to have same columns\nX_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1, fill_value=0)\n\nrf_model = RandomForestClassifier(random_state=42)\nrf_model.fit(X_train_encoded, y_train)\n\nrf_preds = rf_model.predict(X_test_encoded)\n\nprint(\"ðŸ“Š Random Forest Report:\")\nprint(classification_report(y_test, rf_preds))\n\nrf_cm = confusion_matrix(y_test, rf_preds)\nsns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues')\nplt.title(\"Random Forest Confusion Matrix\")\nplt.show()\n\naccuracy = accuracy_score(y_test, rf_preds)\nprint(\"Accuracy:\", accuracy)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#2_Logistic Regression\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Encode categorical features\nX_train_encoded = pd.get_dummies(X_train)\nX_test_encoded = pd.get_dummies(X_test)\n\n# Align columns of train and test sets\nX_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1, fill_value=0)\n\nlog_model = LogisticRegression(max_iter=1000)\nlog_model.fit(X_train_encoded, y_train)\n\nlog_preds = log_model.predict(X_test_encoded)\n\nprint(\"ðŸ“Š Logistic Regression Report:\")\nprint(classification_report(y_test, log_preds))\n\nlog_cm = confusion_matrix(y_test, log_preds)\nsns.heatmap(log_cm, annot=True, fmt='d', cmap='Purples')\nplt.title(\"Logistic Regression Confusion Matrix\")\nplt.show()\n\naccuracy = accuracy_score(y_test, log_preds)\nprint(\"Accuracy:\", accuracy)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#3_LSTM\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\n\n#=============================\n# 1. Encode target variable if categorical\n#=============================\nif y_train.dtype == 'object':\n    le = LabelEncoder()\n    y_train = le.fit_transform(y_train)\n    y_test = le.transform(y_test)\n\n#=============================\n# 2. Encode categorical features\n#=============================\nX_train_encoded = pd.get_dummies(X_train)\nX_test_encoded = pd.get_dummies(X_test)\n\n# Align train/test columns\nX_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1, fill_value=0)\n\n#=============================\n# 3. Scale features\n#=============================\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_encoded)\nX_test_scaled = scaler.transform(X_test_encoded)\n\n#=============================\n# 4. Reshape for LSTM [samples, timesteps, features]\n#=============================\n# à¦à¦–à¦¾à¦¨à§‡ timesteps=1 à¦¦à§‡à¦“à§Ÿà¦¾ à¦¹à¦²à§‹ (à¦¯à¦¦à¦¿ multi-step sequence à¦²à¦¾à¦—à¦²à§‡ à¦ªà¦°à¦¿à¦¬à¦°à§à¦¤à¦¨ à¦•à¦°à§‹)\nX_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\nX_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n\n#=============================\n# 5. Build LSTM model\n#=============================\nmodel = Sequential()\nmodel.add(LSTM(64, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]), activation='tanh'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation='sigmoid'))  # Binary classification; multi-class à¦¹à¦²à§‡ softmax\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n#=============================\n# 6. Train model\n#=============================\nhistory = model.fit(X_train_lstm, y_train,\n                    epochs=20,\n                    batch_size=16,\n                    validation_data=(X_test_lstm, y_test),\n                    verbose=1)\n\n#=============================\n# 7. Predictions\n#=============================\ny_pred_prob = model.predict(X_test_lstm)\ny_pred = (y_pred_prob > 0.5).astype(int)\n\n#=============================\n# 8. Evaluation\n#=============================\nprint(\"ðŸ“Š LSTM Report:\")\nprint(classification_report(y_test, y_pred))\n\nlstm_cm = confusion_matrix(y_test, y_pred)\nsns.heatmap(lstm_cm, annot=True, fmt='d', cmap='Oranges')\nplt.title(\"LSTM Confusion Matrix\")\nplt.show()\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#4_GRU Model Code\nimport numpy as np\nimport tensorflow as tf\nimport random\n\n# 1. Fix seeds for reproducibility\nseed = 42\nnp.random.seed(seed)\ntf.random.set_seed(seed)\nrandom.seed(seed)\n\n# 2. Your other imports and code\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Embedding, GRU, Dense, Dropout\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n# 3. Your dataset and preprocessing\ntexts = [\n    # Positive examples (20)\n    'this is a great movie', 'i really enjoyed this film', 'the movie was fantastic', 'absolutely brilliant plot',\n    'i loved this movie', 'one of the best films ever', 'superb acting by the cast', 'a truly moving story',\n    'highly recommend this picture', 'wonderful experience from start to finish', 'two thumbs up', 'amazing direction',\n    'a must-see film for everyone', 'the visuals were stunning', 'excellent screenplay', 'perfect in every way',\n    'i was captivated throughout', 'an unforgettable journey', 'the soundtrack was beautiful', 'masterfully crafted',\n    # Negative examples (20)\n    'this movie was terrible', 'i did not like the film', 'the acting was awful', 'a complete waste of time',\n    'i hated every moment of it', 'the plot was boring and predictable', 'worst film of the year', 'do not watch this',\n    'a disappointing experience', 'the script was very weak', 'i fell asleep halfway through', 'a total failure',\n    'the characters were one-dimensional', 'it was incredibly slow', 'nothing special about it', 'i want my money back',\n    'a confusing and messy plot', 'the ending was horrible', 'poorly executed idea', 'I would not recommend it'\n]\nlabels = np.array([1]*20 + [0]*20)\n\ntokenizer = Tokenizer(num_words=2000, oov_token=\"<unk>\")\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\nmax_length = max(len(seq) for seq in sequences)\npadded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n\nX_train, X_test, y_train, y_test = train_test_split(\n    padded_sequences, labels, test_size=0.2, random_state=seed\n)\n\nvocab_size = len(tokenizer.word_index) + 1\nembedding_dim = 32\n\nmodel = Sequential([\n    Input(shape=(max_length,)),\n    Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n    GRU(units=64),\n    Dropout(0.5),\n    Dense(units=1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\nhistory = model.fit(X_train, y_train,\n                    epochs=50,\n                    validation_data=(X_test, y_test),\n                    callbacks=[early_stopping],\n                    verbose=2)\n\nloss, accuracy = model.evaluate(X_test, y_test, verbose=0)\nprint(f\"Final Test Accuracy: {accuracy*100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Ensure X_train / X_test are DataFrames\nif isinstance(X_train, np.ndarray):\n    X_train = pd.DataFrame(X_train)\nif isinstance(X_test, np.ndarray):\n    X_test = pd.DataFrame(X_test)\n\n# Now safe to handle categoricals\ncategorical_cols = X_train.select_dtypes(include=['object']).columns\n\nif len(categorical_cols) > 0:\n    X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols)\n    X_test_encoded = pd.get_dummies(X_test, columns=categorical_cols)\n    X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1, fill_value=0)\nelse:\n    X_train_encoded, X_test_encoded = X_train, X_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#5_XGBoost\nimport xgboost as xgb\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Handle categorical only\ncategorical_cols = X_train.select_dtypes(include=['object']).columns\nif len(categorical_cols) > 0:\n    X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols)\n    X_test_encoded = pd.get_dummies(X_test, columns=categorical_cols)\n    X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1, fill_value=0)\nelse:\n    X_train_encoded, X_test_encoded = X_train, X_test\n\n# Convert to DMatrix\ndtrain = xgb.DMatrix(X_train_encoded, label=y_train)\ndtest = xgb.DMatrix(X_test_encoded, label=y_test)\n\n# Parameters\nparams = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss',\n    'eta': 0.1,\n    'max_depth': 6,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'seed': 42\n}\n\n# Train model\nxgb_model = xgb.train(params, dtrain, num_boost_round=200,\n                      evals=[(dtest, \"Test\")], early_stopping_rounds=20)\n\n# Predict\nxgb_preds_prob = xgb_model.predict(dtest)\nxgb_preds = (xgb_preds_prob > 0.5).astype(int)\n\n# Evaluation\nprint(\"ðŸ“Š XGBoost Report:\")\nprint(classification_report(y_test, xgb_preds))\n\nxgb_cm = confusion_matrix(y_test, xgb_preds)\nsns.heatmap(xgb_cm, annot=True, fmt='d', cmap='Greens')\nplt.title(\"XGBoost Confusion Matrix\")\nplt.show()\n\naccuracy = accuracy_score(y_test, xgb_preds)\nroc_auc = roc_auc_score(y_test, xgb_preds_prob)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"ROC-AUC: {roc_auc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}